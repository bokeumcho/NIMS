[info] Saved config to runs/exp_kt2/config.json
[info] Saved config to runs/exp_kt2/config.json[info] Saved config to runs/exp_kt2/config.json[info] Saved config to runs/exp_kt2/config.json


31510 6752 6753
31510 6752 6753
31510 6752 6753
31510 6752 6753
Epoch 1/1 - Training:   0%|          | 0/492 [00:00<?, ?it/s]Epoch 1/1 - Training:   0%|          | 0/492 [00:00<?, ?it/s]Epoch 1/1 - Training:   0%|          | 0/492 [00:00<?, ?it/s]Epoch 1/1 - Training:   0%|          | 0/492 [00:00<?, ?it/s]Epoch 1/1 - Training:   0%|          | 0/492 [01:35<?, ?it/s]
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1285, in _try_get_data
[rank1]:     data = self._data_queue.get(timeout=timeout)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/queue.py", line 180, in get
[rank1]:     self.not_empty.wait(remaining)
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 359, in wait
[rank1]:     gotit = waiter.acquire(True, timeout)
[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank1]:     _error_if_any_worker_fails()
[rank1]: RuntimeError: DataLoader worker (pid 2260084) is killed by signal: Killed. 

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/work/team3/bokeum/main_steps_parallel.py", line 145, in <module>
[rank1]:     for x, y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} - Training"):
[rank1]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
[rank1]:     for obj in iterable:
[rank1]:                ^^^^^^^^
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 734, in __next__
[rank1]:     data = self._next_data()
[rank1]:            ^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1492, in _next_data
[rank1]:     idx, data = self._get_data()
[rank1]:                 ^^^^^^^^^^^^^^^^
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1444, in _get_data
[rank1]:     success, data = self._try_get_data()
[rank1]:                     ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1298, in _try_get_data
[rank1]:     raise RuntimeError(
[rank1]: RuntimeError: DataLoader worker (pid(s) 2260084) exited unexpectedly
Epoch 1/1 - Training:   0%|          | 0/492 [01:37<?, ?it/s]
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1285, in _try_get_data
[rank2]:     data = self._data_queue.get(timeout=timeout)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/queue.py", line 180, in get
[rank2]:     self.not_empty.wait(remaining)
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 359, in wait
[rank2]:     gotit = waiter.acquire(True, timeout)
[rank2]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank2]:     _error_if_any_worker_fails()
[rank2]: RuntimeError: DataLoader worker (pid 2260185) is killed by signal: Killed. 

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/work/team3/bokeum/main_steps_parallel.py", line 145, in <module>
[rank2]:     for x, y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} - Training"):
[rank2]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
[rank2]:     for obj in iterable:
[rank2]:                ^^^^^^^^
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 734, in __next__
[rank2]:     data = self._next_data()
[rank2]:            ^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1492, in _next_data
[rank2]:     idx, data = self._get_data()
[rank2]:                 ^^^^^^^^^^^^^^^^
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1444, in _get_data
[rank2]:     success, data = self._try_get_data()
[rank2]:                     ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1298, in _try_get_data
[rank2]:     raise RuntimeError(
[rank2]: RuntimeError: DataLoader worker (pid(s) 2260185) exited unexpectedly
Epoch 1/1 - Training:   0%|          | 0/492 [01:38<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1285, in _try_get_data
[rank0]:     data = self._data_queue.get(timeout=timeout)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/queue.py", line 180, in get
[rank0]:     self.not_empty.wait(remaining)
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 359, in wait
[rank0]:     gotit = waiter.acquire(True, timeout)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank0]:     _error_if_any_worker_fails()
[rank0]: RuntimeError: DataLoader worker (pid 2260149) is killed by signal: Killed. 

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/work/team3/bokeum/main_steps_parallel.py", line 145, in <module>
[rank0]:     for x, y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} - Training"):
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
[rank0]:     for obj in iterable:
[rank0]:                ^^^^^^^^
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 734, in __next__
[rank0]:     data = self._next_data()
[rank0]:            ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1492, in _next_data
[rank0]:     idx, data = self._get_data()
[rank0]:                 ^^^^^^^^^^^^^^^^
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1444, in _get_data
[rank0]:     success, data = self._try_get_data()
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1298, in _try_get_data
[rank0]:     raise RuntimeError(
[rank0]: RuntimeError: DataLoader worker (pid(s) 2260149) exited unexpectedly
Epoch 1/1 - Training:   0%|          | 0/492 [01:58<?, ?it/s]
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1285, in _try_get_data
[rank3]:     data = self._data_queue.get(timeout=timeout)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/queue.py", line 180, in get
[rank3]:     self.not_empty.wait(remaining)
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 359, in wait
[rank3]:     gotit = waiter.acquire(True, timeout)
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank3]:     _error_if_any_worker_fails()
[rank3]: RuntimeError: DataLoader worker (pid 2260073) is killed by signal: Killed. 

[rank3]: The above exception was the direct cause of the following exception:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/work/team3/bokeum/main_steps_parallel.py", line 145, in <module>
[rank3]:     for x, y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} - Training"):
[rank3]:                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/tqdm/std.py", line 1181, in __iter__
[rank3]:     for obj in iterable:
[rank3]:                ^^^^^^^^
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 734, in __next__
[rank3]:     data = self._next_data()
[rank3]:            ^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1492, in _next_data
[rank3]:     idx, data = self._get_data()
[rank3]:                 ^^^^^^^^^^^^^^^^
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1444, in _get_data
[rank3]:     success, data = self._try_get_data()
[rank3]:                     ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1298, in _try_get_data
[rank3]:     raise RuntimeError(
[rank3]: RuntimeError: DataLoader worker (pid(s) 2260073) exited unexpectedly
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 61, in _pin_memory_loop
    do_one_step()
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 37, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/multiprocessing/reductions.py", line 541, in rebuild_storage_fd
    fd = df.detach()
         ^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/connection.py", line 519, in Client
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 61, in _pin_memory_loop
    do_one_step()
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 37, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/multiprocessing/reductions.py", line 541, in rebuild_storage_fd
    fd = df.detach()
         ^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/connection.py", line 519, in Client
    c = SocketClient(address)
        ^^^^^^^    ^c = SocketClient(address)^
^^^^^^^^^^ ^ ^ 
     File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/connection.py", line 647, in SocketClient
  ^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/connection.py", line 647, in SocketClient
    s.connect(address)
ConnectionRefusedError: [Errno 111] Connection refused
    s.connect(address)
ConnectionRefusedError: [Errno 111] Connection refused
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 61, in _pin_memory_loop
    do_one_step()
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 37, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/multiprocessing/reductions.py", line 541, in rebuild_storage_fd
    fd = df.detach()
         ^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/connection.py", line 519, in Client
    c = SocketClient(address)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/connection.py", line 647, in SocketClient
    s.connect(address)
ConnectionRefusedError: [Errno 111] Connection refused
Exception in thread Thread-2 (_pin_memory_loop):
Traceback (most recent call last):
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 1075, in _bootstrap_inner
    self.run()
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/threading.py", line 1012, in run
    self._target(*self._args, **self._kwargs)
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 61, in _pin_memory_loop
    do_one_step()
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py", line 37, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/multiprocessing/reductions.py", line 541, in rebuild_storage_fd
    fd = df.detach()
         ^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/connection.py", line 519, in Client
    c = SocketClient(address)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/multiprocessing/connection.py", line 647, in SocketClient
    s.connect(address)
ConnectionRefusedError: [Errno 111] Connection refused
[rank0]:[W829 05:31:05.798327060 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0829 05:31:12.207000 2259313 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2259434 closing signal SIGTERM
W0829 05:31:12.210000 2259313 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2259435 closing signal SIGTERM
W0829 05:31:12.211000 2259313 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 2259437 closing signal SIGTERM
E0829 05:31:21.907000 2259313 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 2259436) of binary: /home/work/team3/miniconda3/envs/py312cu121/bin/python3.12
Traceback (most recent call last):
  File "/home/work/team3/miniconda3/envs/py312cu121/bin/torchrun", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/work/team3/miniconda3/envs/py312cu121/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/home/work/team3/bokeum/main_steps_parallel.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-29_05:31:12
  host      : main1
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2259436)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
